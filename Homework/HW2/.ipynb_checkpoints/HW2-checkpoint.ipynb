{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords  #pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from gensim.models import word2vec  #pip install word2vec\n",
    "from wordcloud import WordCloud  #pip install wordcloud\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from textblob import TextBlob  #Sentiment Analysis - pip install textblob\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "path_to_csv = '../../../../cs82_advanced_machine_learning_data/HW2/papers.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Plotting wordcloud to get an idea of important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv(path_to_csv)\n",
    "features=['title']\n",
    "papers=papers.loc[:,features]\n",
    "\n",
    "#removing undesired words from the data \n",
    "papers['title'] = papers['title'].str.replace('Abstract Missing','')\n",
    "papers['title'] = papers['title'].str.replace('Using','')\n",
    "papers['title'] = papers['title'].str.replace('using','')\n",
    "papers['title'] = papers['title'].str.replace(\"New\",'')\n",
    "papers['title'] = papers['title'].str.replace(\"Based\",'')\n",
    "papers['title'] = papers['title'].str.replace('Use','')\n",
    "papers['title'] = papers['title'].str.replace('Used','')\n",
    "papers['title'] = papers['title'].str.replace('Method','')\n",
    "papers['title'] = papers['title'].str.replace('Problem','')\n",
    "papers['title'] = papers['title'].str.replace('Approach','')\n",
    "papers['title'] = papers['title'].str.replace('Model','')\n",
    "papers['title'] = papers['title'].str.replace('Models','')\n",
    "papers['title'] = papers['title'].str.replace('via','')\n",
    "\n",
    "papers=papers.sample(frac=1,random_state=0)\n",
    "train_qs = pd.Series(papers['title'].tolist()).astype(str)\n",
    "\n",
    "qs_text = \"\".join(train_qs)\n",
    "\n",
    "cloud =WordCloud(font_path=None, width=800, height=600, margin=2, ranks_only=None, \n",
    "                 prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, \n",
    "                 min_font_size=4, stopwords=None, random_state=None, background_color='white', \n",
    "                 max_font_size=None, font_step=1, mode='RGB', relative_scaling=.5, regexp=None, \n",
    "                 collocations=True, colormap=None, normalize_plurals=bool, contour_width=0, \n",
    "                 contour_color='black', repeat=None).generate(str(qs_text))\n",
    "print(cloud)\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.imshow(cloud);\n",
    "plt.axis('off');\n",
    "#Word Clouds on a image - https://github.com/amueller/word_cloud/blob/master/examples/alice_colored.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Running t-SNE model and plot to try to find clusters of terms that would help determine topic names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For t-SNE model and plot, we have considered papers \"title\" and papers \"abstract\".\n",
    "papers = pd.read_csv(path_to_csv)\n",
    "features=['title','abstract']\n",
    "\n",
    "#Removing undesired words that have been used frequently in the papers \"abstract\". \n",
    "papers['abstract'] = papers['abstract'].str.replace('Abstract Missing','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('using','')\n",
    "papers['abstract'] = papers['abstract'].str.replace(\"show\",'')\n",
    "papers['abstract'] = papers['abstract'].str.replace(\"based\",'')\n",
    "papers['abstract'] = papers['abstract'].str.replace('use','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('used','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('method','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('problem','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('approach','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('provide','')\n",
    "papers['abstract'] = papers['abstract'].str.replace('model','')\n",
    "\n",
    "papers=papers.loc[:,features]\n",
    "papers.loc[:,'title'] = papers.title.apply(lambda x: x.lower())\n",
    "papers.loc[:,'abstract'] = papers.abstract.apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove chars that are not letters or numbers\n",
    "regex = re.compile('\\n')\n",
    "papers.loc[:,'title'] = papers.title.apply(lambda x: regex.sub(' ',x))\n",
    "papers.loc[:,'abstract'] = papers.abstract.apply(lambda x: regex.sub(' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "stops = set(stopwords.words(\"english\")) #stops\n",
    "stops = stops.union(['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.loc[:,'title'] = papers['title'].apply(lambda x: x.split(' ')) \n",
    "papers.loc[:,'title'] = papers['title'].apply(lambda x: [word for word in x if word not in stops])\n",
    "\n",
    "papers.loc[:,'abstract'] = papers['abstract'].apply(lambda x: x.split(' ')) \n",
    "papers.loc[:,'abstract'] = papers['abstract'].apply(lambda x: [word for word in x if word not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['title','abstract']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            corpus.append(sentence[1])\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(papers)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, size=200, window=10, min_count=500, workers=4, seed=82)\n",
    "model.corpus_count\n",
    "\n",
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in sorted(model.wv.vocab):\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=50, n_components=2, init='pca', method='exact', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.xlabel('TSNE Component1')\n",
    "    plt.ylabel('TSNE Component2')\n",
    "    plt.show()\n",
    "    \n",
    "tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NMF for topic modeling and t-SNE for 2D-embedding\n",
    "\n",
    "We have used 2D-embeddings to visualize the content of all NIPS papers until 2017. In doing so, we have used the method in [1] as a benchmark.\n",
    "\n",
    "We have choosen the topics for clusters from our WordCloud analysis in the section 1 above for the words with higher appearance in NIPS papers till 2017. These topics are: \n",
    "### neural network, bayesian, clustering, optimization, learning, kernel, artificial, reinforcement, image.\n",
    "\n",
    "[1]. https://www.kaggle.com/rjhere23/nips-papers-visualized-with-nmf-and-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = sqlite3.connect('/Users/behrouz/Desktop/Harvard/Week 2/Class 2/section2/data/section2/database.sqlite')\n",
    "\n",
    "#papers = pd.read_sql_query(\"select * from papers;\", conn)\n",
    "#papers.columns = ['id','year','title','event_type','pdf_name','abstract','paper_text']\n",
    "papers = pd.read_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For this analysis we will use \"Papers Text\" to identify the growth of the topics from 1997 until 2017. \n",
    "\n",
    "n_features = 1000\n",
    "n_topics = 9\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,max_features=n_features,stop_words='english')\n",
    "\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(papers['paper_text'])\n",
    "\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=0,alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_embedding = nmf.transform(tfidf)\n",
    "nmf_embedding = (nmf_embedding - nmf_embedding.mean(axis=0))/nmf_embedding.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['neural network',\n",
    "          'bayesian',\n",
    "          'clustering',\n",
    "          'optimization',\n",
    "          'learning',\n",
    "          'kernel',\n",
    "          'artificial',\n",
    "          'reinforcement',\n",
    "          'image']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=3211)\n",
    "tsne_embedding = tsne.fit_transform(nmf_embedding)\n",
    "tsne_embedding = pd.DataFrame(tsne_embedding,columns=['x','y'])\n",
    "tsne_embedding['hue'] = nmf_embedding.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###code used to create the plot for getting the colors \n",
    "#plt.style.use('ggplot')\n",
    "colors = np.array([[ 0.89411765,  0.10196079,  0.10980392,  1. ],\n",
    "                   [ 0.22685121,  0.51898501,  0.66574396,  1. ],\n",
    "                   [ 0.38731259,  0.57588621,  0.39148022,  1. ],\n",
    "                   [ 0.7655671 ,  0.38651289,  0.37099578,  1. ],\n",
    "                   [ 1.        ,  0.78937332,  0.11607843,  1. ],\n",
    "                   [ 0.75226453,  0.52958094,  0.16938101,  1. ],\n",
    "                   [ 0.92752019,  0.48406   ,  0.67238756,  1. ],\n",
    "                   [ 0.60000002,  0.60000002,  0.60000002,  1. ],\n",
    "                   [ 0.51898501,  0.22685121,  0.92752019,  1. ]])\n",
    "\n",
    "legend_list = []\n",
    "\n",
    "for i in range(len(topics)):   \n",
    "    color = colors[i]\n",
    "    legend_list.append(mpatches.Ellipse((0, 0), 1, 1, fc=color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('font',family='monospace')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,2, figsize=(10, 15), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .1, wspace=0)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "count = 0\n",
    "legend = []\n",
    "for year, idx in zip([1992,1997,2002,2007,2012,2017], range(6)):\n",
    "    data = tsne_embedding[papers['year']<=year]\n",
    "    scatter = axs[idx].scatter(data=data,x='x',y='y',s=10,c=data['hue'],cmap=\"Set1\")\n",
    "    axs[idx].set_title('Published Until {}'.format(year),**{'fontsize':'14'})\n",
    "    axs[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"All NIPS proceedings clustered by topic\",**{'fontsize':'14','weight':'bold'})\n",
    "plt.figtext(.51,0.95,'unsupervised topic modeling with NMF based on textual content + 2D-embedding with t-SNE:', **{'fontsize':'10','weight':'light'}, ha='center')\n",
    "#fig.legend(legend_list)\n",
    "fig.legend(legend_list,topics,loc=(0.1,0.89),ncol=3)\n",
    "plt.subplots_adjust(top=0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. FIND TOPICS AND PLOT TOPICS AGAINST TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers = pd.read_csv(path_to_csv)\n",
    "print(df_papers.shape)\n",
    "df_papers.head()\n",
    "df_papers_orig = df_papers.copy()\n",
    "\n",
    "# LOWER CASE\n",
    "df_papers.loc[:,'title'] = df_papers.title.apply(lambda x : x.lower())\n",
    "df_papers.loc[:,'paper_text'] = df_papers.paper_text.apply(lambda x : x.lower())\n",
    "\n",
    "#KEEP ONLY ALPHANUMERIC\n",
    "regex = re.compile(r'\\W+')\n",
    "df_papers.loc[:,'title'] = df_papers.title.apply(lambda x: regex.sub(' ', x))\n",
    "df_papers.loc[:,'paper_text'] = df_papers.paper_text.apply(lambda x: regex.sub(' ', x))\n",
    "\n",
    "#CONVERT TO BOW\n",
    "df_papers.loc[:,'title'] = df_papers['title'].apply(lambda x: x.split(' '))\n",
    "df_papers.loc[:,'paper_text'] = df_papers['paper_text'].apply(lambda x: x.split(' '))\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "stops = stops.union(set(\"year\"))\n",
    "\n",
    "#REMOVE STOP WORDS\n",
    "df_papers.loc[:,'title'] = df_papers['title'].apply(lambda x: [word for word in x if word not in stops])\n",
    "df_papers.loc[:,'paper_text'] = df_papers['paper_text'].apply(lambda x: [word for word in x if word not in stops])\n",
    "\n",
    "# INCREASE WEIGHT ON THE WORDS USED IN THE TITLES BY COUNTING THEM 4 times\n",
    "title_overcount_factor = 3\n",
    "\n",
    "def build_corpus(data): \n",
    "    corpus = []\n",
    "    for index, row in data.iterrows():\n",
    "        title = []\n",
    "        for i in range(title_overcount_factor):\n",
    "            title = row['title'] + title\n",
    "        content = title + row['paper_text']\n",
    "        corpus.append(\" \".join(content))\n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(df_papers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(corpus)\n",
    "print (\"There are %i documents to analyze.\" % n_samples)\n",
    "n_features = 2000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "n_features = 2000\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(corpus)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"\\nThe shape of our count vector matrix: \",tf.shape)\n",
    "#print(tf_vectorizer.get_feature_names()[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 9\n",
    "n_top_words = 20\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_topic_distrib = lda.transform(tf)\n",
    "print(doc_topic_distrib.shape)\n",
    "\n",
    "top_n_titles = 5\n",
    "for i in range(n_components):\n",
    "    idx=doc_topic_distrib[:,i].argsort()[::-1][:top_n_titles]\n",
    "    doc_topic_distrib[idx]\n",
    "    print(\"Topic %d\" %i )\n",
    "    print(df_papers_orig.loc[idx,'title'].values)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0:'Kernel',\n",
    "               1:'Reinforcement',\n",
    "               2:'topic_2',\n",
    "               3:'topic_3',\n",
    "               4:'topic_4',\n",
    "               5:'topic_5',\n",
    "               6:'Neural Network',\n",
    "               7:'topic_7',\n",
    "               8:'Image Recognition',\n",
    "              }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_tfidf.shape)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = [value for key, value in topic_names.items()]\n",
    "df_topics = pd.DataFrame(doc_topic_distrib, columns=topic_list)\n",
    "print(df_topics.shape)\n",
    "df_topics.head()\n",
    "df_topics['year'] = df_tfidf['year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_year = df_topics.groupby('year').mean()\n",
    "topics = set(topic_list)\n",
    "plot_trend(df_topics_year, topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Use TFIDF to find the important terms for each year and get a first impression of the trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS FOR TFIDF\n",
    "min_ngram = 2\n",
    "max_ngram = 2\n",
    "\n",
    "max_df = 0.90\n",
    "min_df = 2\n",
    "max_features = 500\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(min_ngram, max_ngram), max_features = max_features, max_df = max_df)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.get_shape())\n",
    "\n",
    "#print((vectorizer.get_feature_names()))\n",
    "#print(X.has_sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TFIDF RESULTS TO PANDAS DATA FRAME\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(df_tfidf.shape)\n",
    "# ADD THE YEAR COLUMN TO THE DATAFRAME\n",
    "df_tfidf['year'] = df_papers['year']\n",
    "df_tfidf['count'] = 1\n",
    "print(df_tfidf.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROUP BY YEAR\n",
    "\n",
    "#GET THE NUMBER OF PAPERS FOR EACH YEAR \n",
    "counts = df_tfidf.groupby(['year']).agg(['count'])['count']\n",
    "\n",
    "#TAKE THE MEAN IDF SCORE FOR EACH FEATURE\n",
    "df_tfidf_year = df_tfidf.groupby(['year']).mean()\n",
    "\n",
    "#ADD THE COUNT COLUMN FOR EACH YEAR FOR FUTURE USE\n",
    "df_tfidf_year['count'] = counts\n",
    "df_tfidf_year.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS FOR PLOTTING\n",
    "topN_topics = 25  # PLOT THE TOPICS THAT FEATURED IN THE TOPN OF EACH YEAR\n",
    "min_year = 2013   # THE MIN BOUND FOR THE YEARS TO LOOK AT TO FIND THE TOP TOPICS\n",
    "max_year = 2017   # THE MAX BOUND FOR THE YEARS TO LOOK AT TO FIND THE TOP TOPICS\n",
    "\n",
    "\n",
    "# ITERATE THROUGH EACH YEAR AND PICK THE TOP (top_n) TOPICS FROM EACH YEAR TO PLOT\n",
    "years = df_tfidf_year.index\n",
    "print(years)\n",
    "topics  = set()\n",
    "for year in years:\n",
    "        if (year >= min_year) & (year <=max_year):\n",
    "            tfidf_sorted = df_tfidf_year.T.sort_values(by=[year], ascending=False)\n",
    "            topics = topics.union(set(tfidf_sorted.index[:topN_topics]))\n",
    "\n",
    "print(len(topics))\n",
    "\n",
    "# REMOVE TOPICS THAT ARE KNOWN TO BE NOT RELATED TO MACHINE LEARNING\n",
    "remove_set = set([\"using\", \"et al\", \"count\", \"based\", \"via\", \"semi\", \"online\", \n",
    "                  \"large\", \"multi\", \"analysis\", \"data\", \"stochastic\", \"log log\", \"low rank\"])\n",
    "\n",
    "# FINAL LIST OF TOPICS TO PLOT\n",
    "topics = topics.difference(remove_set)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for topic in topics:\n",
    "    plt.plot(df_tfidf_year.index, df_tfidf_year.loc[:,topic])\n",
    "plt.xticks(df_tfidf_year.index)\n",
    "plt.legend(loc = 'upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT ONLY TERMS THAT HAVE A CONSISTENTLY RISING TREND\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "def upward_trend (column):\n",
    "        upward_trend = df_tfidf_year.loc[2017,column] > df_tfidf_year.loc[2016, column] > df_tfidf_year.loc[2015, column] \n",
    "        return upward_trend\n",
    "\n",
    "def downward_trend(column):\n",
    "        downward_trend = df_tfidf_year.loc[2017,column] < df_tfidf_year.loc[2016, column] < df_tfidf_year.loc[2015, column]< \\\n",
    "        df_tfidf_year.loc[2014, column] \n",
    "        return downward_trend\n",
    "\n",
    "\n",
    "topics_upward = set()\n",
    "topics_downward = set()\n",
    "for column in df_tfidf_year.columns:\n",
    "    #print(df_tfidf_year.loc[2017,column])\n",
    "    #print(df_tfidf_year.loc[2016,column]) \n",
    "    if upward_trend(column) :\n",
    "            topics_upward.add(column)\n",
    "    elif downward_trend(column):\n",
    "            topics_downward.add(column)\n",
    "       \n",
    "\n",
    "    \n",
    "# REMOVE TOPICS THAT ARE KNOWN TO BE NOT RELATED TO MACHINE LEARNING\n",
    "remove_set = set([\"systems pages\", \"neural network\", \"related work\", \"conference neural\", \"mini batch\" \n",
    "                   \"log log\", \"low rank\", \"end end\", \"international conference\", \"arxiv preprint\", \n",
    "                  \"related work\", \"preprint arxiv\", \"two different\", \"long term\" , \"fixed point\", \"error rates\", \"standard deviation\",\n",
    "                  'et al', 'kernel methods',  'supported part', 'multi task', 'total number', \n",
    "                  'two dimensional', 'cifar 10', 'natural language', 'error rates', 'task learning', 'artificial neural', 'editors advances', \n",
    "                  'standard deviation', 'count', 'pattern analysis', 'optimal policy', 'reward function', \n",
    "                  'deep neural', 'number iterations', 'analysis machine', 'kernel function', 'recurrent neural', 'state action', 'gaussian kernel', \n",
    "                  'joint distribution', 'function approximation', 'input space', 'international conference', 'number parameters', 'proceedings ieee', \n",
    "                  'proposed method', 'real data', 'information processing', 'domain adaptation', \n",
    "                  'conference computer', 'learning methods', 'model based', 'x0 x0', 'processing systems', 'end end', 'advances neural', \n",
    "                  'neural network', 'density estimation', 'al 2016', 'conference neural', 'theoretical analysis', 'see figure', \n",
    "                  'ground truth', 'linear combination', 'state art', 'synthetic data', 'theoretical results', 'cost function', 'see fig', 'recent work', \n",
    "                  'arxiv preprint', 'high probability', 'results shown', 'experimental results', 'systems pages', 'labeled data', \n",
    "                  'training data', 'two different', 'mini batch', 'preprint arxiv', 'would like', 'neural information', 'conference learning', \n",
    "                  'mean squared', 'prior knowledge', 'machine intelligence','system nips', '40 50', 'fixed point', 'non gaussian', 'long term', 'non parametric', 'information theoretic', 'related work'\n",
    "                  'neural computation', 'ij ij', 'gaussian noise', '20 30', 'principal component', '15 20', 'given set', 'xi xj', '13 14', 'mixture model', 'vector machine', 'small number', 'message passing', \n",
    "                  'high dimensional', 'covariance matrix', 'nips pages', 'described section', 'firing rate', 'mit press', 'previous work', 'diagonal matrix', '16 17', \n",
    "                  'em algorithm', 'learning rule', 'markov random', 'data points', 'technical report', '100 150', '20 40', 'least squares', 'many applications', 'fig shows', 'graphical model', \n",
    "                  'learning problems', 'edu abstract', 'gaussian mixture', '40 60', 'number clusters', '20 10', 'one dimensional', 'random fields',\n",
    "                  'feature selection', 'state space', 'sample size', 'non zero', 'graphical models', 'science university', 'partition function', 'xt xt', \n",
    "                  'main result', 'special case'\n",
    "                 \n",
    "                 ])\n",
    "        \n",
    "# FINAL LIST OF TOPICS TO PLOT\n",
    "topics_upward = topics_upward.difference(remove_set)\n",
    "topics_downward = topics_downward.difference(remove_set)\n",
    "\n",
    "#topics = set([\"computer vision\", \"deep learning\", \"neural networks\", \"value function\", \"pattern recognition\",\n",
    "#               \"reinforcement learning\" ,\"information processing\"])\n",
    "\n",
    "\n",
    "\n",
    "def plot_trend(df, topics, title):\n",
    "    plt.figure(figsize=(20,8))\n",
    "    plt.title(title, fontsize=25)\n",
    "    for topic in topics:\n",
    "        plt.plot(df.index, df.loc[:,topic])\n",
    "    plt.xticks(df.index)\n",
    "    plt.legend(loc = 'upper left')\n",
    "print(\"UPWARD:\")\n",
    "print(topics_downward)\n",
    "print(\"DOWNWARD:\")\n",
    "print(topics_downward)\n",
    "\n",
    "plot_trend(df_tfidf_year, topics_upward, \"Topics trending UPWARDS for the last 3 years\")\n",
    "plot_trend(df_tfidf_year, topics_downward,  \"Topics trending DOWNWARDS for the last 3 years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
