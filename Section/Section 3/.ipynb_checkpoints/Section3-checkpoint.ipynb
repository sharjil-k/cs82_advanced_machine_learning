{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "\n",
    "------\n",
    "\n",
    "\n",
    "CSCI E-82 - Advanced Machine Learning, Data Mining and Artificial Intelligence\n",
    "=====\n",
    "\n",
    "# Section 3:  Saturday 22 September 10am EDT\n",
    "\n",
    "*Dave Dowey*\n",
    "\n",
    "--------\n",
    "\n",
    "### [Text processing pipeline](#text_pipeline)\n",
    "\n",
    "\n",
    "### [Latent Dirichlet Allocation](#lda)\n",
    "- Simple example\n",
    "- IMDB movie review topics\n",
    "\n",
    "\n",
    "### [Earth Mover Distance (EMD) & Word Mover Distance (WMD)](#wmd)\n",
    "- IMDB movie review similarity\n",
    "\n",
    "\n",
    "### [Frequent Itemsets](#frequent_itemsets)\n",
    "- Frequent Itemsets, Association Rules, \n",
    "- Support, Confidence, Lift\n",
    "- Instacart Example\n",
    "- IMDB example for compound words\n",
    "\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "Loading of some librairies and some code for Matplotlib styles\n",
    "\n",
    "If you have not already installed them:\n",
    "\n",
    "* `pip install nltk`\n",
    "* either `conda install gensim` or `pip install gensim`\n",
    "* `pip install pyemd`\n",
    "\n",
    "and \n",
    "\n",
    "* `pip install mlxtend`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shkhan2\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyemd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-44399c9941c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m  \u001b[1;31m#either: conda install gensim or: pip install gensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyemd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memd\u001b[0m   \u001b[1;31m# pip install pyemd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyemd'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "# Text pipeline and NLP packages\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords  #pip install nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from gensim import corpora, models \n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.models import word2vec  #either: conda install gensim or: pip install gensim\n",
    "\n",
    "from pyemd import emd   # pip install pyemd\n",
    "\n",
    "\n",
    "\n",
    "# SciKit NMF and LDA package and some easy text data\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Python package for Apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder  #pip install mlxtend\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import ast\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='text_pipeline'></a>\n",
    "\n",
    "[back to top](#top)\n",
    "\n",
    "\n",
    "# Text processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First - we download some interesting text data from the Large Movie Database (IMDB). This is usually used for classification problems since it has ratings from bad (0) to good (10). We won't use the labelled data, but rather the dataset for unsupervised classification that is part of the \"train\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source:\n",
    "\n",
    "The [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)\n",
    "\n",
    "From: **Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).**  [bib](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.bib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning - Incoming ~80 Mb Dataset will be stored in your current working folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvfz aclImdb_v1.tar.gz;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: we are only using the \\*.txt files in the '/aclImdb/train/unsup/' folder. You can delete the other folders, if you want, or keep them if you want to use them later for some classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = os.getcwd()\n",
    "\n",
    "documents = []\n",
    "for i in range(100):\n",
    "    \n",
    "    filename = mydir + \"/aclImdb/train/unsup/\"+ str(i) + \"_0.txt\"\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        [documents.append(l) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizer\n",
    "Let's use the tokeniser to split out the words - we also transform everything to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "doc_lower = [doc.lower() for doc in documents]\n",
    "doc_tokens = [tokenizer.tokenize(doc) for doc in doc_lower]\n",
    "\n",
    "print(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "And now let's take out english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\")) #stops\n",
    "\n",
    "#print(stops,\"\\n\") #Already defined in NLTK\n",
    "stops = stops.union(['i','br'])  # add some stopwords\n",
    "\n",
    "stopped_doc_tokens = []\n",
    "for doc in doc_tokens:\n",
    "    stopped_doc_tokens.append([word for word in doc if not word in stops])\n",
    "\n",
    "print(stopped_doc_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "And stemming words to reduce topically similar words to their root. For example, “stemming,” “stemmer,” “stemmed,” reduced to “stem.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopstem_doc_tokens = []\n",
    "for doc in stopped_doc_tokens:\n",
    "    stopstem_doc_tokens.append([PorterStemmer().stem(word) for word in doc])\n",
    "\n",
    "print(len(stopstem_doc_tokens))\n",
    "print(stopstem_doc_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "What if we want a list of all the words in all the documents?\n",
    "\n",
    "A double list comprehension is a pythonic way of doing it  \n",
    "`[word for sentence in text for word in sentence]`\n",
    "\n",
    "The we use `set()` to give us just the unique words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = set([word for sentence in stopstem_doc_tokens for word in sentence])\n",
    "print(allwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim dictionary \n",
    "\n",
    "We can also use the gemsim module corpora to generate a dictionary which gives the set of all tokens and attributes to them a term ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(stopstem_doc_tokens)\n",
    "\n",
    "print(dictionary.token2id)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in stopstem_doc_tokens]\n",
    "print(corpus[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([[(dictionary[id], freq) for id, freq in cp] for cp in corpus[:1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='lda'></a>\n",
    "\n",
    "[back to top](#top)\n",
    "\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA’s Generative Model (lecture slide)\n",
    "\n",
    "- To create a new document:\n",
    "- Determine the number of words in the document\n",
    "  * Choose a percentage mixture of topics (these need to add to 100%)\n",
    "  * Create words until have you have the number of desired words\n",
    "    * Sampling a topic according to the multinomial mixture of topics\n",
    "    * Sample a word based on topics’ multinomial distribution\n",
    "- No syntax or order but still topics\n",
    "\n",
    "-----\n",
    "\n",
    "### Algorithm going backwards (lecture slide)\n",
    "- Start with a corpus of documents = $D$\n",
    "- Select the number of topics = $K$\n",
    "- Randomly assign each word in each document to one of topics $K$\n",
    "- For each document $d$ & multiple epochs\n",
    " - For each word $w$ in $d$\n",
    "   - For each topic $t$\n",
    "     * Compute PTD = Prob (topic $t$ | document $d$)\n",
    "        * % words assigned that topic in that doc\n",
    "     * Compute PWT = Prob (word $w$ | topic $t$)\n",
    "        * % assignments to that topic for word $w$ over corpus\n",
    "   - Assign word $w$ to new topic maximizing PTD\\*PWT\n",
    "     - ~Probability of having a topic $t$ generating word $w$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model\n",
    "\n",
    "Now we want to use gensim to generate an LDA model. The syntax for the module is simple: \n",
    "\n",
    "Parameters:   \n",
    "\n",
    "`corpus` ({iterable of list of (int, float), scipy.sparse.csc}, optional) - Stream of document vectors or sparse matrix of shape (num_terms, num_documents). If not given, the model is left untrained.\n",
    "\n",
    "`num_topics` (int, optional) - The number of requested latent topics to be extracted from the training corpus. This will generally depend on your problem and the number of documents in the set to be analyzed.\n",
    "\n",
    "`id2word` ({dict of (int, str), gensim.corpora.dictionary.Dictionary}, required) The dictionary that is used for mapping term ID to the tokens/terms. It is used to determine the vocabulary size.  \n",
    "\n",
    "`passes` (int, optional) - The number of times the model passes through the corpus on training.\n",
    "\n",
    "And - directly from the documentation (see the Lecture notes for motivation)\n",
    "\n",
    "`alpha` ({numpy.ndarray, str}, optional) – Can be set to an 1D array of length equal to the number of expected topics that expresses our a-priori belief for the each topics’ probability. Alternatively default prior selecting strategies can be employed by supplying a string:\n",
    "\n",
    "- ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / topicno.\n",
    "- ’auto’: Learns an asymmetric prior from the corpus.\n",
    "\n",
    "`eta` ({float, np.array, str}, optional) -  A-priori belief on word probability, this can be:\n",
    "\n",
    "- scalar for a symmetric prior over topic/word probability,\n",
    "- vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "- matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "- the string ‘auto’ to learn the asymmetric prior from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LDA](LDA.png)\n",
    "\n",
    "![matrices](matrices.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=6, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `print_topics` method to print out the results - specifying the number of topics that we want to see, and the number of words per topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(lda.print_topics(num_topics=6, num_words=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well have we done?  \n",
    "We can print out some elements from the model to indicate the measure of how good the topic allocation is\n",
    "\n",
    "- perplexity  - the lower the better\n",
    "- coherence score  - the higher the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
    "\n",
    "# coherence score\n",
    "coherence_model = CoherenceModel(model=lda, texts=stopstem_doc_tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What topic is allocated to a given document (let's say the 8th one)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopstem_doc_tokens[8])\n",
    "topic = lda.get_document_topics(corpus[8])\n",
    "print(topic)\n",
    "lda.print_topic(topic[0][0], topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the parameters $\\alpha$ and $\\eta$ ?\n",
    "\n",
    "It would seem that both are set by Gensim to symmetric priors of `1.0 / num_topics`  as a default. An alternative asymmetric prior would be `1.0 / topic index` for the topic probability, or numpy vectors or matrices ($\\eta$) of probabilities for each topic or word-topic combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=6, id2word = dictionary, passes=20, alpha=1.0/6.0*np.ones((6)), eta=None)\n",
    "pp.pprint(lda.print_topics(num_topics=6, num_words=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=6, id2word = dictionary, passes=20, alpha='asymmetric', eta='auto')\n",
    "pp.pprint(lda.print_topics(num_topics=6, num_words=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics=6, id2word = dictionary, passes=20, alpha='asymmetric', eta=0.25)\n",
    "pp.pprint(lda.print_topics(num_topics=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
    "\n",
    "# coherence score\n",
    "coherence_model = CoherenceModel(model=lda, texts=stopstem_doc_tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full movie review dataset\n",
    "\n",
    "Next, let's load the full unsupervised dataset with all the 50k files of reviews using the wonderful \"glob\" module. We will then do the topic modelling on all the reviews. \n",
    "\n",
    "If you have not used \"glob\" before, please check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = os.getcwd()\n",
    "documents = []\n",
    "\n",
    "files_to_load = glob.glob(\"./aclImdb/train/unsup/*.txt\")\n",
    "for filename in files_to_load:\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        [documents.append(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, process all the documents through the tokenizer-stopword-stemmer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = []\n",
    "for doc in documents:\n",
    "    doc = tokenizer.tokenize(doc.lower())\n",
    "    doc_tokens.append([word for word in doc if not word in stops])\n",
    "    #doc = [word for word in doc if not word in stops]\n",
    "    #doc_tokens.append([PorterStemmer().stem(word) for word in doc])\n",
    "\n",
    "print(len(doc_tokens))\n",
    "print(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_tokens)\n",
    "print(dictionary)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "lda = models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=10)\n",
    "print(\"done in %0.3fs.\" % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(lda.print_topics(num_topics=5, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perplexity\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
    "\n",
    "# coherence score\n",
    "#coherence_model = CoherenceModel(model=lda, texts=doc_tokens, dictionary=dictionary, coherence='c_v')\n",
    "#coherence = coherence_model.get_coherence()\n",
    "#print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with SciKit Learn\n",
    "\n",
    "Let's take the full unsupervised dataset and look at how to do the LDA with SciKit Learn\n",
    "One difference - we won't bother to use the stemmer, but the rest of the pipeline for generating the term frequencies, we do with the SciKit Learn method `CountVectorizer`.\n",
    "\n",
    "We'll use code that is sourced from this [SciKit Learn documentation example](http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the parameters for the LDA:\n",
    "\n",
    "- `n_features` maximum features in CountVectorizer\n",
    "- `n_components` number of topics in the LDA\n",
    "- `n_top_words` number of top words in the topic to print out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(corpus)\n",
    "\n",
    "print (\"There are %i documents to analyze.\" % n_samples)\n",
    "\n",
    "n_features = 2000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\")) #stops\n",
    "stops = stops.union(['i','br'])  # add some stopwords\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stops)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"\\nThe shape of our count vector matrix: \",tf.shape)\n",
    "print(tf_vectorizer.get_feature_names()[:30])\n",
    "\n",
    "tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the `max_df` and `min_df` parameters in `CountVectorizer` mean? See the [StackOverflow answer here](https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a id='wmd'></a>\n",
    "\n",
    "[back to top](#top)\n",
    "\n",
    "# Earth Mover Distance (EMD) & Word Mover Distance (WMD)\n",
    "\n",
    "From [Word Embeddings to Document Distances.  Kusner Sun Kolkin Weinberger 2015](https://dl.acm.org/citation.cfm?id=3045221)\n",
    "\n",
    "[PPT on the technique](https://tao.lri.fr/tiki-download_wiki_attachment.php?attId=1549)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of the algorithm (from Lecture notes)\n",
    "\n",
    "- Remove stop words\n",
    "- Use word2vec vectors so $\\textrm{word}_i \\rightarrow  \\vec x_i $\n",
    "- Distance of words = cost = $c(i,j) = \\| x_i - x_j \\|_2$\n",
    "- Flow matrix T where\n",
    "  * $T_{ij}$ = flow from $d_i$ to $d_j$ so $T_{src-dest}$\n",
    "  * $T_{ij} >= 0$\n",
    "- $d_k$ set to 1/#words for each sentence\n",
    "  * Dividing up word to flow out $\\sum_{j} T_{ij} = d_i$\n",
    "  * Input to word has to add up $\\sum_{i} T_{ij} = d_j$\n",
    "- Distance of sentences = $\\sum_{i,j} T_{ij} \\cdot c(i,j)$\n",
    "  * Min cost to move all words from $d_i$ to $d_j$ \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the simple Obama example mentioned in class. It comes from this [WMD tutorial Jupyter notebook](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentence_obama = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'\n",
    "sentence_obama = sentence_obama.lower().split()\n",
    "sentence_president = sentence_president.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = [w for w in sentence_obama if w not in stops]\n",
    "sentence_president = [w for w in sentence_president if w not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_obama)\n",
    "print(sentence_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "if not os.path.exists('GoogleNews-vectors-negative300.bin.gz'):\n",
    "    !wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following should take a few minutes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "model = models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('Took %.2f seconds to generate the embeddings.' %(time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Make sure that you have installed the pyemd package before the following wmd distance calculation*\n",
    "\n",
    "You may have to reload the notebook (uggh, yes) after installing - if the next cell gives:\n",
    "\n",
    "`ImportError: Please install pyemd Python package to compute WMD.` \n",
    "\n",
    "Then, shut down the Jupyter server and restart it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try two reviews from the movie review database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\" \".join(doc_tokens[45]))\n",
    "print()\n",
    "print (\" \".join(doc_tokens[100]))\n",
    "print()\n",
    "print (\" \".join(doc_tokens[108]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = model.wmdistance(doc_tokens[45], doc_tokens[100])\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = model.wmdistance(doc_tokens[100], doc_tokens[108])\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Aside \n",
    "\n",
    "\n",
    "### Interesting question from last lecture: what is the difference between cosine similarity and Euclidean distance?\n",
    "\n",
    "Recall the definitions:\n",
    "\n",
    "Cosine similarity - angle close to zero means $cos \\theta = 1 $ so similar words have embeddings with cosine similarity close to 1, and words that are not similar have angles at 90 degrees and cosine similarity close to 0.\n",
    "\n",
    "\n",
    "$$ \\textrm{similarity } (a,b) = \\cos \\theta = \\frac{\\vec a  \\cdot \\vec b}{\\| \\vec a  \\|  \\| \\vec b  \\| } $$  \n",
    "\n",
    "\n",
    "Euclidean distance - it is a distance so bigger is less similar, and therefore similar words have embeddings that are close in Euclidean space - i.e. close to 0. Distance is given by the dot product.\n",
    "\n",
    "\n",
    "$$ \\textrm{distance } (a,b) = \\| \\vec a  - \\vec b  \\| = \\sqrt{ \\| \\vec a \\|^2  +   \\| \\vec b \\|^2  - 2 \\vec a \\cdot \\vec b } $$  \n",
    "\n",
    "Where is the linkage? If the vectors (embeddings) are normalized, then the two norms $\\| \\vec a  \\|$ and $ \\| \\vec b  \\|$  are equal to $1$, and the distance becomes    \n",
    "\n",
    "$$ \\textrm{distance } (a,b) = \\sqrt{2} \\cdot \\sqrt{ 1 -  \\vec a \\cdot \\vec b } $$\n",
    "\n",
    "So when $\\cos \\theta = 1$ the distance $\\| \\vec a  - \\vec b  \\| = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_obama = model.wv['obama']\n",
    "vec_president = model.wv['president']\n",
    "\n",
    "cos_obama = cosine_similarity(np.vstack((vec_obama, vec_president)))\n",
    "print(\"calculate the cosine similarity:\")\n",
    "print(cos_obama)\n",
    "\n",
    "euc_obama = euclidean_distances(np.vstack((vec_obama, vec_president)))\n",
    "print(\"\\ncalculate the Euclidean distance:\")\n",
    "print(euc_obama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='frequent_itemsets'></a>\n",
    "\n",
    "\n",
    "[back to top](#top)\n",
    "\n",
    "\n",
    "# Frequent Itemsets / Apriori / Association Rules\n",
    "\n",
    "**Applications**\n",
    "Apart from classic case of finding frequent itemsets in a grocery store enivironment. \n",
    "\n",
    "This method can also be used in following: \n",
    "\n",
    "(1) Detect Plagiarism - Baskets = Sentences; Items = Documents containing those sentences.;\n",
    "    - Items that appear together too often could detect plagiarism. \n",
    "    - Notice items do not have to be \"in\" the baskets.\n",
    "    \n",
    "(2) Baskets = patients; Items = drugs and side effects.\n",
    "    - Has been used to detect combinations of drugs that result in particular side effect. \n",
    "    - But requires extension, Absence of an item should be observed as well as presence. \n",
    "\n",
    "\n",
    "**Frequent Itemsets**  Intuitively, a set of items that appears in many baskets is said to be “frequent.” To be formal, we assume there is a number $s$, called the support threshold. If $I$ is a set of items, the support for $I$ is the number of baskets for which $I$ is a subset. We say $I$ is frequent if its support is $s$ or more.\n",
    "\n",
    "**Support/Support threshold**\n",
    "- Find sets of items that appear together “frequently” in baskets\n",
    "- Support for itemset $I$: Number of baskets containing all items in $I$\n",
    "- Given support threshold $s$, then sets of items that appear in at least s baskets are called ***frequent itemsets***.\n",
    "- The support metric is defined for itemsets, not assocication rules, and computes the proportion of transactions that contain the antecedant A. \n",
    "- Given a rule $A \\rightarrow C$, \n",
    "    * $A$ stands for antecedant and \n",
    "    * $C$ stands for consequent.\n",
    "\n",
    "**Association Rules** \n",
    "Classic case: If someone buys diapers, then he/she is likely to buy beer. $\\{Diapers\\} \\rightarrow \\{Beer\\}$\n",
    "- If-then rules about the contents of the basket.  \n",
    "\n",
    "${i_1, i_2,...i_k } \\rightarrow   j $  means: “if a basket contains all of i1,…,ik then it is likely to contain j.\"\n",
    "In practice there are many rules, want to find significant/interesting ones!\n",
    "\n",
    "**Confidence** of this association rule is the probability of j given I = $\\{i_i,i_2...i_k\\}$\n",
    "$$confidence(I \\rightarrow j) =  \\frac{support(I \\cup J)}{support(I)}$$\n",
    "\n",
    "Note that the metric is not symmetric or directed; for instance, the confidence for $A \\rightarrow C$ is different than the confidence for $C \\rightarrow A$. The confidence is 1 (maximal) for a rule $A \\rightarrow C$ if the consequent and antecedent always occur together.\n",
    "\n",
    "**Lift** \n",
    "$$lift(A\\rightarrow C) = \\frac{confidence(A \\rightarrow C)}{support(C)} \\; range:[0: \\infty] $$\n",
    "\n",
    "The lift metric is commonly used to measure how much more often the antecedent and consequent of a rule  $A \\rightarrow C$ occur together than we would expect if they were statistically independent. If $A$ and $C$ are independent, the Lift score will be exactly $1$.\n",
    "\n",
    "\n",
    "**leverage**\n",
    "\n",
    "$$leverage(A\\rightarrow C))=support(A\\rightarrow C))−support(A) \\times support(C)  \\; \\; range: [−1,1] $$\n",
    "\n",
    "Leverage computes the difference between the observed frequency of $A$ and $C$ appearing together and the frequency that would be expected if $A$ and $C$ were independent. An leverage value of $0$ indicates independence.\n",
    "\n",
    "**conviction**\n",
    "\n",
    "$$ conviction(A\\rightarrow C)= \\frac{1−support(C)}{1−confidence(A\\rightarrow C)} \\; \\; range: [0,\\infty] $$\n",
    "\n",
    "A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes $0$ (due to $1 - 1$) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is $1$.\n",
    "\n",
    "\n",
    "Ref: MMDS Book - http://www.mmds.org/ \n",
    "\n",
    "Source of most of this description:\n",
    "https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLxtend \n",
    "\n",
    "*https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each row is one transaction\n",
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oht = TransactionEncoder()\n",
    "oht_ary = oht.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(oht_ary, columns=oht.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori(df, min_support=0.6 , use_colnames=True)\n",
    "# Itemsets must appear in atleast 60% of the baskets. \n",
    "# We see that [Eggs] appear 4/5 baskets, [Kidney Beans] in 5/5 baskets, [Yogurt] appears 3/5 baskets. \n",
    "# [Eggs, Kidney Beans]  together appear in 4/5 baskets etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the length of itemset\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting\n",
    "frequent_itemsets[ (frequent_itemsets['length'] == 2) &\n",
    "                   (frequent_itemsets['support'] >= 0.8) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence(Eggs->KidneyBeans) = Probability of having Kidney Beans in basket given that eggs are in basket = 0.8/0.8 = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Instacart](https://www.kaggle.com/c/instacart-market-basket-analysis/data) \n",
    "    - Instacart open sourced 3M grocery orders for 200K customers (anonymized). \n",
    "    - Goal of the competition was to predict which products (about 50K products) will be in a user's (75K users) next order.\n",
    "(In this section we will look at Frequent Itemsets and Association Rules of these orders.)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Source:\n",
    "\n",
    "\"The [Instacart Online Grocery Shopping Dataset 2017](https://www.instacart.com/datasets/grocery-shopping-2017)”, Accessed from https://www.instacart.com/datasets/grocery-shopping-2017 on 22 September 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning - Incoming ~200 Mb Dataset will be stored in your current working folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.amazonaws.com/instacart-datasets/instacart_online_grocery_shopping_2017_05_01.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xvfz instacart_online_grocery_shopping_2017_05_01.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_pr = pd.read_csv('./instacart_2017_05_01/order_products__prior.csv', usecols=[0,1])\n",
    "products = pd.read_csv('./instacart_2017_05_01/products.csv', usecols=[0,1])\n",
    "print(order_pr.shape, len(pd.unique(order_pr.order_id)), products.shape)  \n",
    "order_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have about 3.2M distinct orders (baskets). About 50K products. On average each basket contains 10 products. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Following code takes about 3-4 minutes on my mac, saved output in summary_df1.csv\n",
    "a = datetime.now()\n",
    "merged_df = pd.merge(order_pr,products, on='product_id',how='left')\n",
    "summary_df1 = merged_df[['order_id',\n",
    "                        'product_id']].groupby('order_id')['product_id'].apply(list)\n",
    "\n",
    "summary_df1 = pd.DataFrame(summary_df1)\n",
    "summary_df1.reset_index(inplace=True,drop=True)\n",
    "b = datetime.now()\n",
    "print((b-a).seconds)\n",
    "summary_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary_df1.to_csv('summary_df1.csv',index=False)\n",
    "summary_df1 = pd.read_csv('summary_df1.csv',index_col=None)\n",
    "summary_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df1.shape #3.2M orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2017)\n",
    "summary_df1 = summary_df1.sample(frac=0.05)  #Extracting 5% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df1.shape)\n",
    "summary_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = summary_df1.product_id[0:1000]\n",
    "tmp = np.array(tmp)\n",
    "#print(tmp,\"\\n\")\n",
    "\n",
    "tmp1 = ', '.join(map(str, tmp))\n",
    "tmp1 = ast.literal_eval(tmp1)\n",
    "#print(tmp1,\"\\n\")\n",
    "\n",
    "oht = TransactionEncoder()\n",
    "oht_ary = oht.fit(tmp1).transform(tmp1)\n",
    "df1 = pd.DataFrame(oht_ary, columns=oht.columns_)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df1.columns,columns=['product_id'])\n",
    "df2 = df2.merge(products)\n",
    "df1.columns = df2.product_name\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at all the possible combinations of products in each basket, and count them up...\n",
    "\n",
    "No. Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with considering itemsets that are chosen in 1% of all baskets, and contain up to 2 items\n",
    "\n",
    "This yields 150 itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df1, min_support=0.01 , use_colnames=True, max_len=2) #0.01 = 1% of baskets\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[frequent_itemsets.itemsets.apply(lambda x: 'Organic Gala Apples' in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at row 8 in the association table \n",
    "\n",
    "Confidence(**Bag of Organic Bananas**  --> **Organic Gala Apples**)   \n",
    "\n",
    "= Probability of having **Organic Gala Apples** in basket given that **Bag of Organic Bananas** are in basket  \n",
    "\n",
    "= 0.010/0.024 = 0.41667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df1, min_support=0.01 , use_colnames=True, max_len=2) #0.01 = 1% of baskets\n",
    "#Visualizing frequent itemsets\n",
    "df_plot = frequent_itemsets.sort_values('support', ascending=False)[0:25]\n",
    "df_plot.reset_index(inplace=True,drop=True)\n",
    "df_plot.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f , ax = plt.subplots(figsize=(8, 9))\n",
    "\n",
    "sns.barplot(x = df_plot.support,y=df_plot.index, ax = ax , orient='h');\n",
    "ax.set_yticklabels(df_plot.itemsets);\n",
    "ax.axvline(x=0.01, linestyle='dashed', linewidth=0.7 , color ='r');\n",
    "ax.grid(False);\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False);\n",
    "ax.set_title('25 Most Frequent Single Itemsets (Support = 0.01)');\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing itemsets of length 2\n",
    "frequent_itemsets = apriori(df1, min_support=0.01 , use_colnames=True, max_len=2) #0.01 = 1% of baskets\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets = frequent_itemsets[frequent_itemsets.length==2]\n",
    "frequent_itemsets.sort_values('support',inplace=True, ascending=False)\n",
    "frequent_itemsets.reset_index(inplace=True,drop=True)\n",
    "frequent_itemsets.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f , ax = plt.subplots(figsize=(8, 9))\n",
    "\n",
    "sns.barplot(x = frequent_itemsets.support,y=frequent_itemsets.index, ax = ax , orient='h');\n",
    "ax.set_yticklabels(frequent_itemsets.itemsets);\n",
    "ax.axvline(x=0.01, linestyle='dashed', linewidth=0.7 , color ='r');\n",
    "ax.grid(False);\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False);\n",
    "ax.set_title('25 Most Frequent Itemsets (Length = 2, Support = 0.01)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*https://www.kaggle.com/msp48731/frequent-itemsets-and-association-rules?scriptVersionId=1175233*  - R Script \n",
    "(arules package)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
